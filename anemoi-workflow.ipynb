{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1deeec57-cf88-4f95-92cb-8fd2a1638ce5",
   "metadata": {},
   "source": [
    "# Anemoi Training Workflow Demo\n",
    "\n",
    "This notebook will guide you through training an AI4NWP model with the Anemoi framework. https://github.com/ecmwf/anemoi-core/tree/main\n",
    "\n",
    "Datasets used for training will be created using the ufs2arco package. https://github.com/NOAA-PSL/ufs2arco/tree/main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0e3243-f89b-4771-be86-880d7bbe5eab",
   "metadata": {},
   "source": [
    "# 1) Environment Setup\n",
    "\n",
    "The environment tested with this notebook utilized Python 3.11.13 on Ubuntu 24.04. \n",
    "- **There is no guarantee that this notebook will run error-free using a Python installation on Windows**.\n",
    "\n",
    "We will utilize *pip* for installing required packages. Make sure you have the latest version of *pip* before proceeding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b744034-41ae-4894-853f-924a6ebd41d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/andrewjustin/miniconda3/envs/anemoi-ufs2arco/lib/python3.11/site-packages (25.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48698173-1cc9-427f-8486-1866c6a11b9c",
   "metadata": {},
   "source": [
    "There are several packages that we need to install through pip.\n",
    "- *ufs2arco*: module that will be used to generate the datasets. https://github.com/NOAA-PSL/ufs2arco/tree/main\n",
    "- *anemoi-datasets*: Anemoi package that optimizes and handles datasets. https://anemoi.readthedocs.io/projects/datasets/en/latest/\n",
    "    - Note that you *can* generate datasets with *anemoi-datasets* instead of *ufs2arco*, however this is not recommended.\n",
    "- *anemoi-graphs*: Anemoi package that allows you to design graphs for AI4NWP models. https://anemoi.readthedocs.io/projects/graphs/en/latest/\n",
    "- *anemoi-models*: provides the rest of the Anemoi packages with core model components. https://anemoi.readthedocs.io/projects/models/en/latest/\n",
    "- *anemoi-training*: provides the training functionality for Anemoi. https://anemoi.readthedocs.io/projects/training/en/latest/\n",
    "- *anemoi-inference*: framework for performing model inference with AI4NWP models trained using Anemoi. https://anemoi.readthedocs.io/projects/inference/en/latest/\n",
    "- *flash-attn*: Attention mechanism used in Anemoi's transformer models.\n",
    "  - Flash attention ONLY works on **NVIDIA Ampere GPUs *or newer***. An exhaustive list of Ampere GPUs can be found here: https://en.wikipedia.org/wiki/Ampere_(microarchitecture)#Products_using_Ampere\n",
    "- *mpi4py*: Python bindings for the MPI interface. This is only required if you plan to retrieve data in parallel (**strongly recommended**, especially for very large datasets)\n",
    "- *trimesh*: allows models to utilize triangular meshes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd9ae2ce-d6e4-425d-bcb9-a7564cf51a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ufs2arco==0.6.0\n",
      "  Using cached ufs2arco-0.6.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting anemoi-datasets\n",
      "  Using cached anemoi_datasets-0.5.25-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting anemoi-graphs\n",
      "  Using cached anemoi_graphs-0.6.2-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting anemoi-models\n",
      "  Using cached anemoi_models-0.8.1-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting anemoi-training\n",
      "  Using cached anemoi_training-0.5.1-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting anemoi-inference\n",
      "  Using cached anemoi_inference-0.6.3-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting flash-attn\n",
      "  Downloading flash_attn-2.8.1.tar.gz (8.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting wget\n",
      "  Using cached wget-3.2-py3-none-any.whl\n",
      "Collecting mpi4py\n",
      "  Using cached mpi4py-4.1.0-cp311-cp311-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (16 kB)\n",
      "Collecting numpy<2.3\n",
      "  Using cached numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting earthkit-data<0.14.0\n",
      "  Using cached earthkit_data-0.13.9-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting xarray (from ufs2arco==0.6.0)\n",
      "  Downloading xarray-2025.7.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting cf_xarray (from ufs2arco==0.6.0)\n",
      "  Using cached cf_xarray-0.10.6-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting cftime (from ufs2arco==0.6.0)\n",
      "  Using cached cftime-1.6.4.post1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.7 kB)\n",
      "Collecting netCDF4 (from ufs2arco==0.6.0)\n",
      "  Using cached netCDF4-1.7.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting h5netcdf (from ufs2arco==0.6.0)\n",
      "  Using cached h5netcdf-1.6.3-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting zarr<3 (from ufs2arco==0.6.0)\n",
      "  Using cached zarr-2.18.7-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting cfgrib (from ufs2arco==0.6.0)\n",
      "  Using cached cfgrib-0.9.15.0-py3-none-any.whl.metadata (55 kB)\n",
      "Collecting bottleneck (from ufs2arco==0.6.0)\n",
      "  Using cached bottleneck-1.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
      "Collecting dask[complete] (from ufs2arco==0.6.0)\n",
      "  Using cached dask-2025.5.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fsspec (from ufs2arco==0.6.0)\n",
      "  Using cached fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting s3fs (from ufs2arco==0.6.0)\n",
      "  Using cached s3fs-2025.5.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting gcsfs (from ufs2arco==0.6.0)\n",
      "  Using cached gcsfs-2025.5.1-py2.py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting array-api-compat (from earthkit-data<0.14.0)\n",
      "  Using cached array_api_compat-1.12.0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting deprecation (from earthkit-data<0.14.0)\n",
      "  Using cached deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting earthkit-meteo>=0.0.1 (from earthkit-data<0.14.0)\n",
      "  Using cached earthkit_meteo-0.4.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting eccodes>=1.7 (from earthkit-data<0.14.0)\n",
      "  Using cached eccodes-2.42.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (14 kB)\n",
      "Collecting entrypoints (from earthkit-data<0.14.0)\n",
      "  Using cached entrypoints-0.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting filelock (from earthkit-data<0.14.0)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting jinja2 (from earthkit-data<0.14.0)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting jsonschema (from earthkit-data<0.14.0)\n",
      "  Using cached jsonschema-4.24.0-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting lru-dict (from earthkit-data<0.14.0)\n",
      "  Using cached lru_dict-1.3.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
      "Collecting markdown (from earthkit-data<0.14.0)\n",
      "  Using cached markdown-3.8.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting multiurl>=0.3.3 (from earthkit-data<0.14.0)\n",
      "  Using cached multiurl-0.3.5-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting pandas (from earthkit-data<0.14.0)\n",
      "  Downloading pandas-2.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
      "Collecting pdbufr>=0.11 (from earthkit-data<0.14.0)\n",
      "  Using cached pdbufr-0.14.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting pyyaml (from earthkit-data<0.14.0)\n",
      "  Using cached PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting tqdm>=4.63 (from earthkit-data<0.14.0)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting asciitree (from zarr<3->ufs2arco==0.6.0)\n",
      "  Using cached asciitree-0.3.3-py3-none-any.whl\n",
      "Collecting fasteners (from zarr<3->ufs2arco==0.6.0)\n",
      "  Using cached fasteners-0.19-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting numcodecs!=0.14.0,!=0.14.1,<0.16,>=0.10.0 (from zarr<3->ufs2arco==0.6.0)\n",
      "  Using cached numcodecs-0.15.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.9 kB)\n",
      "Collecting deprecated (from numcodecs!=0.14.0,!=0.14.1,<0.16,>=0.10.0->zarr<3->ufs2arco==0.6.0)\n",
      "  Using cached Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting anemoi-transform>=0.1.10 (from anemoi-datasets)\n",
      "  Using cached anemoi_transform-0.1.11-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting anemoi-utils>=0.4.21 (from anemoi-utils[provenance]>=0.4.21->anemoi-datasets)\n",
      "  Downloading anemoi_utils-0.4.28-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting cfunits (from anemoi-datasets)\n",
      "  Using cached cfunits-3.3.7-py3-none-any.whl\n",
      "Collecting semantic-version (from anemoi-datasets)\n",
      "  Using cached semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting zarr<3 (from ufs2arco==0.6.0)\n",
      "  Using cached zarr-2.18.4-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting hydra-core>=1.3 (from anemoi-graphs)\n",
      "  Using cached hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting matplotlib>=3.6 (from anemoi-graphs)\n",
      "  Using cached matplotlib-3.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting networkx>=3.1 (from anemoi-graphs)\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting numpy<2.3\n",
      "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting plotly>=5.19 (from anemoi-graphs)\n",
      "  Using cached plotly-6.2.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting scikit-learn>=1.5 (from anemoi-graphs)\n",
      "  Using cached scikit_learn-1.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (17 kB)\n",
      "Collecting torch>=2.2 (from anemoi-graphs)\n",
      "  Using cached torch-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Collecting torch-geometric>=2.3.1 (from anemoi-graphs)\n",
      "  Using cached torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
      "Collecting typeguard>=4 (from anemoi-graphs)\n",
      "  Using cached typeguard-4.4.4-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting einops>=0.6.1 (from anemoi-models)\n",
      "  Using cached einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting datashader>=0.17 (from anemoi-training)\n",
      "  Using cached datashader-0.18.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting mlflow>=2.11.1 (from anemoi-training)\n",
      "  Using cached mlflow-3.1.1-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting pydantic>=2.9 (from anemoi-training)\n",
      "  Using cached pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Collecting pynvml>=11.5 (from anemoi-training)\n",
      "  Using cached pynvml-12.0.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyshtools>=4.13 (from anemoi-training)\n",
      "  Using cached pyshtools-4.13.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
      "Collecting pytorch-lightning>=2.1 (from anemoi-training)\n",
      "  Using cached pytorch_lightning-2.5.2-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting timm>=0.9.2 (from anemoi-training)\n",
      "  Downloading timm-1.0.17-py3-none-any.whl.metadata (59 kB)\n"
     ]
    }
   ],
   "source": [
    "!pip install ufs2arco==0.6.0 anemoi-datasets==0.5.25 anemoi-graphs==0.6.2 anemoi-models==0.8.1 anemoi-training==0.5.1 anemoi-inference==0.6.3 flash-attn mpi4py trimesh 'numpy<2.3' 'earthkit-data<0.14.0' --force-reinstall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f2d5fa-e962-4bce-8ff3-c03ec072addc",
   "metadata": {},
   "source": [
    "# 2) Build Dataset Recipes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ffbde1-358c-4dab-afec-c86294a7cfcc",
   "metadata": {},
   "source": [
    "Datasets for Anemoi are created using the ufs2arco package: https://github.com/NOAA-PSL/ufs2arco/tree/main\n",
    "\n",
    "YAML files containing a 'recipe' for the dataset can be called to generate the datasets.\n",
    "\n",
    "You can create a single, large dataset for training, validation, and testing, or you can separate these into their own datasets.\n",
    "\n",
    "We will walk through the process of creating a recipe YAML file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7f199e-1771-42d3-89aa-2fa7e191846b",
   "metadata": {},
   "source": [
    "## 2.1) Define data mover\n",
    "\n",
    "As the name suggests, the data mover will be used to move data from a remote location to some local directory. There are two datamovers that can be used: 'datamover' and 'mpidatamover' (requires *mpi4py* package). The only difference between these is that 'mpidatamover' has the ability to utilize multiple processor threads, allowing data to be retrieved in parallel. \n",
    "\n",
    "The data mover is defined in the YAML recipe with the **mover.name** parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d923b8-96e6-41c6-99e7-81010be17502",
   "metadata": {},
   "outputs": [],
   "source": [
    "mover:\n",
    "    name: mpidatamover"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d683984c-1761-4490-b89b-6f903a6ff177",
   "metadata": {},
   "source": [
    "## 2.2) Define directories\n",
    "\n",
    "There are three directory parameters that must be specified in the YAML file:\n",
    "- **directories.zarr**: directory for the dataset in zarr format\n",
    "- **directories.cache**: directory for dataset cache\n",
    "- **directories.logs**: directory for logs showing dataset progress. These logs can be useful for monitoring dataset progress and debugging\n",
    "\n",
    "Note that recursive directory structures will automatically be created if they do not already exist.\n",
    "\n",
    "An example implementation in a YAML recipe is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b15a79-be97-4143-8998-a8b1951f5bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "directories:\n",
    "  zarr: p1/dataset/training.zarr\n",
    "  cache: p1/dataset/cache\n",
    "  logs: p1/dataset/logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248f3a47-ae80-4ba2-a5f6-1b9263dbc69c",
   "metadata": {},
   "source": [
    "## 2.3) Define dataset configuration\n",
    "\n",
    "The dataset configuration will be within **source**.\n",
    "\n",
    "### 2.3.1) Data source, time window, ensemble member selection\n",
    "\n",
    "Choose the data source and time window you would like to use. The parameters required for this are as follows:\n",
    "- **source.name**: name of the dataset\n",
    "- **source.uri**: URI of the dataset\n",
    "- **source.time.start**: beginning of the desired time window with format YYYY-MM-DD[T]HH\n",
    "- **source.time.end**: end of the time window with format YYYY-MM-DD[T]HH\n",
    "- **source.time.freq**: timestep frequency\n",
    "\n",
    "If your dataset has forecast hours (e.g., GFS), you can specify desired forecast hours:\n",
    "- **source.fh.start**: beginning forecast hour\n",
    "- **source.fh.end**: end forecast hour\n",
    "- **source.fh.step**: forecast hour step/interval\n",
    "\n",
    "If your dataset has ensemble members (e.g., GEFS), you can retrieve specific ensemble members:\n",
    "- **source.member.start**: beginning member number\n",
    "- **source.member.end**: end member number\n",
    "- **source.member.step**: member number step/interval\n",
    "\n",
    "An example implementation in a YAML recipe is shown below. Note that we will add more **source** parameters in subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae845ff-24c7-4846-a68e-01b753121dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "source:\n",
    "  name: gcs_replay_atmosphere\n",
    "  uri: gs://noaa-ufs-gefsv13replay/ufs-hr1/0.25-degree-subsampled/03h-freq/zarr/fv3.zarr\n",
    "  time:\n",
    "    start: 1994-01-01T00\n",
    "    end: 1994-01-31T21\n",
    "    freq: 3h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a55acd-b6f3-4363-90fe-877f1b7b3e30",
   "metadata": {},
   "source": [
    "### 2.3.2) Variables\n",
    "\n",
    "All variables are defined with the **source.variables** parameter in the recipe YAML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417b67e5-acf3-41b8-aa77-c6ac7e08f7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "source:\n",
    "  name: gcs_replay_atmosphere\n",
    "  uri: gs://noaa-ufs-gefsv13replay/ufs-hr1/0.25-degree-subsampled/03h-freq/zarr/fv3.zarr\n",
    "  time:\n",
    "    start: 1994-01-01T00\n",
    "    end: 1994-01-31T21\n",
    "    freq: 3h\n",
    "\n",
    "  variables:\n",
    "    - tmp2m\n",
    "    - spfh2m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816d308b-f50a-49fe-9133-a638f87dac36",
   "metadata": {},
   "source": [
    "### 2.3.3) Pressure Levels\n",
    "\n",
    "Pressure levels can be explicitly defined, or you can select a set of pressure levels through slicing in the recipe YAML.\n",
    "- **source.levels**: list of all desired pressure levels\n",
    "- **source.slice.sel.levels**: retrieve a 'slice' of all pressure levels between two values (e.g., [200, 1000] grabs all pressure levels between 1000 and 250 hPa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420719a8-44e9-40b4-a98f-7261ad29ceda",
   "metadata": {},
   "outputs": [],
   "source": [
    "source:\n",
    "  name: gcs_replay_atmosphere\n",
    "  uri: gs://noaa-ufs-gefsv13replay/ufs-hr1/0.25-degree-subsampled/03h-freq/zarr/fv3.zarr\n",
    "  time:\n",
    "    start: 1994-01-01T00\n",
    "    end: 1994-01-31T21\n",
    "    freq: 3h\n",
    "\n",
    "  variables:\n",
    "    - tmp2m\n",
    "    - spfh2m\n",
    "\n",
    "  slices:\n",
    "    sel:\n",
    "      level: [200, 1000]  # hPa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8277e212-049e-45ec-94ab-e8452f020584",
   "metadata": {},
   "source": [
    "### 2.3.4) Coordinates and Selecting Subdomains\n",
    "\n",
    "By default, all lat/lon points in the desired dataset will be obtained and no arguments are required to acquire the entire grid. Lat/lon coordinates in a subdomain can be explicitly defined, or you can select sets of coordinates through slicing in the recipe YAML.\n",
    "- **source.longitude**: list of all desired longitude points\n",
    "- **source.latitude**: list of all desired latitude points\n",
    "- **source.slice.sel.longitude**: retrieve a slice of all longitudes within a range (e.g., [200, 300] grabs all longitudes between 200 and 300 degrees east, using the 360 degree system)\n",
    "- **source.slice.sel.latitude**: retrieve a slice of all latitude values (e.g., [51, 25] grabs all latitudes between 25 and 51 degrees north)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009419c4-2b5c-40e9-badc-6c6148828531",
   "metadata": {},
   "outputs": [],
   "source": [
    "source:\n",
    "  name: gcs_replay_atmosphere\n",
    "  uri: gs://noaa-ufs-gefsv13replay/ufs-hr1/0.25-degree-subsampled/03h-freq/zarr/fv3.zarr\n",
    "  time:\n",
    "    start: 1994-01-01T00\n",
    "    end: 1994-01-31T21\n",
    "    freq: 3h\n",
    "\n",
    "  variables:\n",
    "    - tmp2m\n",
    "    - spfh2m\n",
    "\n",
    "  slices:\n",
    "    sel:\n",
    "      level: [200, 1000]  # hPa\n",
    "      latitude: [53, 21]\n",
    "      longitude: [225, 300]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e20523-5361-4544-b1fa-bb7b5524b882",
   "metadata": {},
   "source": [
    "### 2.3.5) Configure Outputs\n",
    "\n",
    "All of the outputs in the recipe YAML are done in the **target** section.\n",
    "\n",
    "- **target.name**: target name (unsure of the exact purpose)\n",
    "- **target.sort_channels_by_levels**: setting this to True will sort the channels by pressure level\n",
    "- **target.rename**: allows you to rename variables or coordinates (example usage below)\n",
    "- **target.chunks**: configure chunks by coordinates (example usage below)\n",
    "- **target.forcings**: list of forcing variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035aab77-e860-4076-b406-654023ed14ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "target:\n",
    "  name: forecast\n",
    "  sort_channels_by_levels: True\n",
    "  rename:\n",
    "    level: pressure  # rename 'level' to 'pressure'\n",
    "\n",
    "  chunks:\n",
    "    time: 1  # one timestep per chunk\n",
    "    variable: -1  # undefined (all variables in one chunk)\n",
    "    ensemble: 1  # one ensemble member per chunk\n",
    "\n",
    "  forcings:\n",
    "    - cos_latitude\n",
    "    - sin_latitude\n",
    "    - cos_longitude\n",
    "    - sin_longitude\n",
    "    - cos_julian_day\n",
    "    - sin_julian_day\n",
    "    - cos_local_time\n",
    "    - sin_local_time\n",
    "    - cos_solar_zenith_angle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd0951d-33ba-42dd-a60e-5d81ebf36dd5",
   "metadata": {},
   "source": [
    "### 2.3.6) Transforms\n",
    "\n",
    "One neat feature of Anemoi is its support for data transformations prior to saving, all of which is done in the **transforms** section. You can perform mathematical operations on a variable(s) in order to get the desired units.\n",
    "\n",
    "- **transforms.divide**: divide a specified variable by some value (example in the cell below)\n",
    "- **transforms.multiply**: multiple a specified variable by some value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "871a95fe-cafd-4248-830a-71c30e7bfbb7",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (836770422.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mtransforms:\u001b[39m\n               ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "transforms:\n",
    "  divide:\n",
    "    geopotential_at_surface: 9.80665  # converts geopotential (m2/s2) to geopotential height (m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8248ae-882c-4865-9006-e3610b7b8dfe",
   "metadata": {},
   "source": [
    "## 2.4) Generate datasets with ufs2arco\n",
    "\n",
    "ufs2arco is used to build the datasets. You can keep training, validation, and testing datasets as the same file and select time windows from the zarr file, or you can make separate zarr files for each dataset. Separating dataset files might come with costs and benefits, but this is largely up to user preference.\n",
    "\n",
    "Given a recipe *training.yaml*, you can generate the dataset with the command below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56e0581-25f9-4c46-8479-fc32d384cd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ufs2arco training.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205f4a5f-bfe5-444c-8192-24dff163d48d",
   "metadata": {},
   "source": [
    "# 3) Generate and Modify Config Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a69f2e1-ae5b-43ec-aac8-52bf948b8c91",
   "metadata": {},
   "source": [
    "## 3.1) Generate Config Files\n",
    "\n",
    "Anemoi has a command that generates some config files which can be utilized during model training.\n",
    "\n",
    "Note that these generated files have **a lot** of parameters that should be modified in order to streamline your model training workflow.\n",
    "\n",
    "Run the command below and **carefully** read the instructions/documentation in this sections 3.2 - 3.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de01ad47-a98e-4428-b12f-d5b3a84f9bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "!anemoi-training config generate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6455de1-b5e3-420d-aa3e-58eaa497776a",
   "metadata": {},
   "source": [
    "## 3.2) Define batch sizes and configure datasets\n",
    "\n",
    "Batch sizes must be defined for each dataset. The default *dataloader* file *dataloader/native_grid.yaml* has pre-defined batch sizes, however these can be overriden in *config.yaml*.\n",
    "- **dataloader.batch_size.training**: training dataset batch size\n",
    "- **dataloader.batch_size.validation**: validation dataset batch size\n",
    "- **dataloader.batch_size.test**: testing dataset batch size\n",
    "\n",
    "For each dataset, the dataset path and start and end dates need to be specified.\n",
    "- **dataloader.training.dataset**: full path to the training dataset\n",
    "- **dataloader.training.start**: start date for training dataset (YYYY-MM-DD)\n",
    "- **dataloader.training.end**: end date for training dataset (YYYY-MM-DD)\n",
    "- **dataloader.validation.dataset**: full path to the validation dataset\n",
    "- **dataloader.validation.start**: start date for validation dataset (YYYY-MM-DD)\n",
    "- **dataloader.validation.end**: end date for validation dataset (YYYY-MM-DD)\n",
    "- **dataloader.test.dataset**: full path to the test dataset\n",
    "- **dataloader.test.start**: start date for test dataset (YYYY-MM-DD)\n",
    "- **dataloader.test.end**: end date for test dataset (YYYY-MM-DD)\n",
    "\n",
    "Example implementation in *config.yaml*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1d45ee-0e0f-401a-8ecc-5e91cdb662e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader:\n",
    "  batch_size:\n",
    "    training: 2\n",
    "    validation: 2\n",
    "    test: 2\n",
    "  training:\n",
    "    dataset: ${hardware.paths.data}/training.zarr\n",
    "    start: 1994-01-01\n",
    "    end: 1994-01-31\n",
    "  validation:\n",
    "    dataset: ${hardware.paths.data}/validation.zarr\n",
    "    start: 1994-02-01\n",
    "    end: 1994-02-28\n",
    "  test:\n",
    "    dataset: ${hardware.paths.data}/testing.zarr\n",
    "    start: 1994-03-01\n",
    "    end: 1994-03-31"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a49d4d-20c0-4380-bccf-d22263497f56",
   "metadata": {},
   "source": [
    "## 3.3) Configure GPUs and Paths\n",
    "\n",
    "One of the most important steps for running the Anemoi framework is configuring paths. At the top of *config.yaml*, the 'hardware' parameter should be set to 'example'. This calls the default settings in *hardware/example.yaml*, however the **data** path is not specified in the *example* yaml. In addition, you may want to specify different directories for storing outputs and model graphs.\n",
    "\n",
    "- **hardware.paths.output**: directory for the outputs (checkpoints, plots, etc.). Directory structure will be created if it does not already exist.\n",
    "- **hardware.paths.data**: directory for the datasets generated with ufs2arco.\n",
    "- **hardware.paths.graph**: directory for the model graph.\n",
    "\n",
    "The name of the zarr file containing the training dataset must also be specified.\n",
    "- **hardware.files.dataset**: name of the training dataset zarr file (do not include absolute path with directory structure)\n",
    "\n",
    "You can also specify the number of GPUs to use for each model with the **hardware.num_gpus_per_model** parameter.\n",
    "\n",
    "An example implementation in *config.yaml* is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041e2885-3cb3-4f68-bf83-4bd7ff5d65f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hardware:\n",
    "\n",
    "  num_gpus_per_model: 1\n",
    "\n",
    "  paths:\n",
    "    output: p1/training-output/\n",
    "    data: p1/dataset\n",
    "    graph: p1/graph\n",
    "\n",
    "  files:\n",
    "    dataset: training.zarr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852ed49c-d159-4b4b-8751-19d78432b833",
   "metadata": {},
   "source": [
    "## 3.4) Configure Model Training\n",
    "\n",
    "There are a few parameters that should be specified in the main *config.yaml* file so model training configurations can be easily modified.\n",
    "\n",
    "At the top of *config.yaml*, you will probably see a 'training' parameter that is set to 'default'. This calls training configuration settings in the *training/default.yaml* file. All of these settings can be overriden in *config.yaml*.\n",
    "\n",
    "Here are some useful training parameters to include in *config.yaml*:\n",
    "- **training.max_epochs**: specifies the maximum number of epochs for model training. Training will stop if this limit is reached.\n",
    "- **training.max_steps**: specifies the maximum number of total steps for model training (*not steps per epoch*). Training will stop if this limit is reached.\n",
    "- **training.lr.rate**: starting learning rate\n",
    "- **training.lr.min**: minimum learning rate\n",
    "\n",
    "An example implementation in *config.yaml* with the aforementioned parameters is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf1895f-b515-4751-820e-8700917cd080",
   "metadata": {},
   "outputs": [],
   "source": [
    "training:\n",
    "  max_epochs: 500\n",
    "  max_steps: 10000\n",
    "  lr:\n",
    "    rate: 1e-4\n",
    "    min: 3e-7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d8c525-c0f6-481c-b7cc-978cd37b2ace",
   "metadata": {},
   "source": [
    "## 3.5) Configure Diagnostics\n",
    "\n",
    "During training, it is useful to plot sample model predictions and log other information pertaining to the model output/performance in order to get a good idea if your model is 'working' as intended.\n",
    "\n",
    "In the *config.yaml* file, the default file for diagnostics is *diagnostics/evaluation.yaml*. There are a couple empty fields that we will need to define in the following steps.\n",
    "\n",
    "### 3.5.1) Performance Logging\n",
    "\n",
    "For now, we will disable Weights and Biases for performance logging (though you may want to configure a WandB workflow in the future). This can be done by setting the **diagnostics.log.wandb.entity** parameter to 'null'.\n",
    "\n",
    "We will also disable the MLflow tracking server by setting **diagnostics.log.mlflow.tracking_uri** to 'null'.\n",
    "\n",
    "An example implementation in *config.yaml* is shown below. Note that we will continue to modify **diagnostics** in later steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ee8437-e6a7-43c7-bd06-98aaccd88d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostics:\n",
    "  log:\n",
    "    wandb:\n",
    "      entity: null\n",
    "    mlflow:\n",
    "      tracking_uri: null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae6c6d9-5db9-4d0a-b35f-52e13402f25c",
   "metadata": {},
   "source": [
    "### 3.5.2) Plotting\n",
    "\n",
    "With the default settings in *diagnostics/evaluation.yaml*, the following plots will be produced at user-defined frequencies for specified variables:\n",
    "* Spatial plots of model predictions and errors\n",
    "* Histograms showing binned model predictions and errors for **every** variable in a single plot\n",
    "\n",
    "The frequency of plotting can be modified directly in *config.yaml* with the following parameters:\n",
    "* **diagnostics.plot.frequency.epoch**: plot frequency in epochs\n",
    "* **diagnostics.plot.frequency.batch**: plot frequency in batches\n",
    "\n",
    "Adding these to **diagnostics** in *config.yaml*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd14c0af-a659-4e3c-8470-496084e99d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostics:\n",
    "  log:\n",
    "    wandb:\n",
    "      entity: null\n",
    "    mlflow:\n",
    "      tracking_uri: null\n",
    "  plot:\n",
    "    frequency:\n",
    "      epoch: 5\n",
    "      batch: 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa01aa95-30a3-499e-bd12-5c415a58311f",
   "metadata": {},
   "source": [
    "The next thing to do is define what variables we want to plot. \n",
    "\n",
    "First, let's modify a few lines in *diagnostics/evaluation.yaml*.\n",
    "- Under **callbacks**, assure that every instance of **parameters** (should be three instances in total) calls back to the user-specified variables in **diagnostics.plot.parameters** (see cell below). This will make sure that plots include every variable that you would like to monitor.\n",
    "- You can leave the instance of **parameters** near the top of the file unchanged as we will be overriding it in *config.yaml*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f432e44-07ff-40b2-9ce3-8c1488e812be",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters: ${diagnostics.plot.parameters}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dc78fb-ee49-4040-8589-e2eb7368a350",
   "metadata": {},
   "source": [
    "Now that the plotting file is configured, we can add define the variables we want to plot in *config.yaml*.\n",
    "* Note that precipitation and related moisture variables need to be defined in **diagnostics.plot.precip_and_related_fields** as well as **diagnostics.plot.parameters**.\n",
    "\n",
    "Adding our desired variables for plotting to **diagnostics.plot** in *config.yaml*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45764717-ad87-4b42-a3fa-264034c368c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostics:\n",
    "  log:\n",
    "    wandb:\n",
    "      entity: null\n",
    "    mlflow:\n",
    "      tracking_uri: null\n",
    "  plot:\n",
    "    frequency:\n",
    "      epoch: 1\n",
    "      batch: 5\n",
    "    parameters:\n",
    "      - tmp_825  # 825 hPa temperature\n",
    "      - tmp2m  # 2-meter temperature\n",
    "    precip_and_related_fields: []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20167b36-f068-43ec-a1ae-4d76e44ddf43",
   "metadata": {},
   "source": [
    "### 3.5.3) Model Settings\n",
    "\n",
    "Several model configuration settings can be changed in *config.yaml*. By default, the *config.yaml* file will use the 'gnn' model. This is the Graph Neural Network architecture (https://arxiv.org/abs/1812.08434). These models are designed for learning relationships between nodes and edges. The two other model configurations available by default are the transformer (https://arxiv.org/abs/1706.03762) and graph transformer (https://arxiv.org/abs/2407.09777). Transformers excel at learning relationships between sequential data, such as sequential forecast timesteps in atmospheric data. The graph transformer combines the ideas of the GNN and transformer to handle sequential data connected through a graph.\n",
    "\n",
    "The *model* files generated by Anemoi all have ReLU (rectified linear unit) boundings applied to the variable 'tp', or total precipitation. The output of the ReLU function $y$ will be zero for a given input $x$ when $x\\leq0$, otherwise $y=x$. This means that the output of ReLU will never be negative, which makes since for precipitation.\n",
    "\n",
    "If you do not have precipitation in your dataset, you need can disable all boundings in *config.yaml* by passing an empty list to the *model.bounding* parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56a9e2e0-c904-4667-86c2-7b9584d7b01c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2674895688.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mmodel:\u001b[39m\n          ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "model:\n",
    "  bounding: []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfba27c-e930-440f-9423-f99f4aa18eee",
   "metadata": {},
   "source": [
    "You can configure other model settings as well, such as the number of channels with **model.num_channels** and the number of layers in the GNN processor with **model.processor.num_layers**.\n",
    "\n",
    "The transformer model uses the Attention mechanism with multiple 'heads' (https://arxiv.org/abs/1706.03762). If you are using the transformer model, the number of heads can be changed with the **model.num_heads** parameter in *config.yaml*. The size of the 'window', or the area that the transformer can see at one time, can be changed with **model.window_size**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6ad9da-5bd8-42d2-a931-9dce5ffd6f8c",
   "metadata": {},
   "source": [
    "# 4) Set Environment Variables\n",
    "\n",
    "Anemoi requires a \"base seed\" and a SLURM job ID.\n",
    "- The base seed is used to initialize model weights. Changing the seed will result in different initial model parameters.\n",
    "- The SLURM job ID is required, even if you are not on SLURM (just leave it as \"0\").\n",
    "\n",
    "*Hydra* can be configured to output more complete tracebacks for debugging purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ead33613-a578-4c21-8e4e-67881aeb2eda",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2674895688.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mmodel:\u001b[39m\n          ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "model:\n",
    "  bounding: []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce40c91-442d-4533-8199-8465cdf61b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "### Required ###\n",
    "os.environ[\"ANEMOI_BASE_SEED\"] = \"42\"\n",
    "os.environ[\"SLURM_JOB_ID\"] = \"0\"\n",
    "\n",
    "### Optional ###\n",
    "os.environ['HYDRA_FULL_ERROR'] = \"1\"  # for debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279cc428-3369-42e4-9fd6-8abc9b8116a3",
   "metadata": {},
   "source": [
    "## 5) Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7463104c-5f43-4b05-848d-5bcfa232d789",
   "metadata": {},
   "outputs": [],
   "source": [
    "!anemoi-training train --config-name=config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17722f48-70fa-4c94-8bcd-88aa6847a41a",
   "metadata": {},
   "source": [
    "## 6) Model Inference\n",
    "\n",
    "Model inference with Anemoi is performed with the *anemoi-inference* module: https://anemoi.readthedocs.io/projects/inference/en/latest/index.html#index-page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bc7ed4-63df-448c-996a-a6401024ce52",
   "metadata": {},
   "source": [
    "### 6.1) Retrieve Model Runs and Load Checkpoint\n",
    "Each model run is saved in a folder with a random hash identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1406d5a0-9f10-4a02-bbf1-d907ea2b0a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "model_runs = os.listdir('p1/training-output/checkpoint')\n",
    "print('Available model runs:')\n",
    "for run in model_runs:\n",
    "    print(run + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dc6472-f2fe-4f08-a94c-715809cf674f",
   "metadata": {},
   "source": [
    "Select a model run from the list above and load the checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd3728f-99ca-42fb-a121-570ca90e220b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_run = 'd46e7b66-9ba1-474f-9142-5dd28be63f50'  # model run hash identifier\n",
    "\n",
    "## Do not change this ##\n",
    "checkpoint = f'p1/training-output/checkpoint/{model_run}/inference-last.ckpt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58da79c3-7478-4b8e-9e54-da2e50896c7f",
   "metadata": {},
   "source": [
    "### 6.2) Configure and Run Model Inference\n",
    "Select a target forecast time (valid time) from the testing dataset and set a forecast lead time.\n",
    "\n",
    "You can also create and call a config YAML file that contains the inference settings, however all settings can be easily passed through the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b11141c-affd-4258-a42f-e9c07498ac3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "forecast_time = '1994-03-31T21'  # valid time [YYYY]-[MM]-[DD]T[HH]\n",
    "lead_time = 12  # hours\n",
    "\n",
    "## Do not change these ##\n",
    "inference_dataset = 'p1/dataset/testing.zarr'\n",
    "output_file = 'forecast.nc'  # output file containing the model forecast\n",
    "\n",
    "!anemoi-inference run checkpoint={checkpoint} date={forecast_time} lead_time={lead_time} input.dataset={inference_dataset} output.netcdf={output_file}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
